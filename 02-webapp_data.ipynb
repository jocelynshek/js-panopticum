{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "git push -u origin main\n",
    "- refine this:\n",
    "    outline process of clustering and sentiment analysis: make sure methodologically solid and produces meaningful results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "more_info = pd.read_feather(\"../../data/more_info_political.feather\")\n",
    "more_info['combined_text'] = (\n",
    "    more_info['main_headline'].fillna('') + ' ' +\n",
    "    more_info['abstract'].fillna('') + ' ' +\n",
    "    more_info['lead_paragraph'].fillna('')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1: new, trump, president, year, said, federal, states, war, world, officials\n",
      "Topic #2: trump, new, president, court, biden, states, year, abortion, climate, united\n",
      "Topic #3: president, trump, harris, kamala, kamala harris, donald, donald trump, biden, vice, vice president\n",
      "Topic #4: israel, gaza, israeli, hamas, war, said, trump, cease, president, military\n",
      "Topic #5: new, trump, president, york, new york, city, year, black, said, people\n",
      "Topic #6: president, trump, new, biden, people, year, election, city, war, said\n",
      "Topic #7: trump, president, donald trump, donald, new, tariffs, said, elect, president elect, states\n",
      "Topic #8: trump, president, new, donald trump, donald, china, war, years, people, said\n",
      "Topic #9: ukraine, russia, president, said, russian, trump, war, new, ukrainian, military\n",
      "Topic #10: israel, said, state, minister, president, biden, prime, prime minister, states, netanyahu\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english', ngram_range=(1,2))\n",
    "X = vectorizer.fit_transform(more_info['combined_text'])\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=10, random_state=0)\n",
    "lda.fit(X)\n",
    "\n",
    "# Print top terms per topic\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = f\"Topic #{topic_idx+1}: \"\n",
    "        message += \", \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "\n",
    "print_top_words(lda, vectorizer.get_feature_names_out(), 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jocelynshek/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Top words for 7 topics ===\n",
      "Topic 1: ['people', 'coal', 'last', 'black', 'like', 'americans', 'life', 'country']\n",
      "Topic 2: ['israel', 'israeli', 'gaza', 'said', 'hezbollah', 'officials', 'lebanon', 'beirut']\n",
      "Topic 3: ['could', 'china', 'tuesday', 'economy', 'strike', 'gulf', 'ports', 'market']\n",
      "Topic 4: ['climate', 'united', 'states', 'world', 'said', 'china', 'global', 'court']\n",
      "Topic 5: ['trump', 'president', 'donald', 'harris', 'biden', 'kamala', 'abortion', 'former']\n",
      "Topic 6: ['ukraine', 'russia', 'east', 'russian', 'president', 'middle', 'first', 'foreign']\n",
      "Topic 7: ['city', 'nuclear', 'plant', 'power', 'mayor', 'help', 'bank', 'west']\n"
     ]
    }
   ],
   "source": [
    "# 1) Install what you need (run once in your venv)\n",
    "# pip install gensim nltk pyldavis\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "import pyLDAvis.gensim_models\n",
    "import pandas as pd\n",
    "\n",
    "# Assume you already have more_info['combined_text']\n",
    "texts_raw = more_info['combined_text'].fillna(\"\")\n",
    "\n",
    "# 2) Preprocess: tokenize, remove stopwords & short tokens\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "def preprocess(doc):\n",
    "    tokens = simple_preprocess(doc, deacc=True)      # lowercase, strip accents/punct\n",
    "    return [t for t in tokens if t not in stop_words and len(t) > 3]\n",
    "\n",
    "texts = texts_raw.map(preprocess).tolist()\n",
    "\n",
    "# 3) Build dictionary & corpus\n",
    "dictionary = Dictionary(texts)\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5, keep_n=5000)\n",
    "corpus = [dictionary.doc2bow(t) for t in texts]\n",
    "\n",
    "# 4) Train & inspect models for different topic counts\n",
    "def train_and_print(k):\n",
    "    model = LdaModel(\n",
    "        corpus=corpus,\n",
    "        id2word=dictionary,\n",
    "        num_topics=k,\n",
    "        random_state=42,\n",
    "        passes=10,\n",
    "        alpha=\"auto\"\n",
    "    )\n",
    "    print(f\"\\n=== Top words for {k} topics ===\")\n",
    "    for i in range(k):\n",
    "        print(f\"Topic {i+1}:\", [w for w,_ in model.show_topic(i, 8)])\n",
    "    return model\n",
    "\n",
    "# Your sweet spot: pick 5–10\n",
    "lda7 = train_and_print(7)\n",
    "\n",
    "# 5) Visualize the 7-topic model interactively\n",
    "#vis = pyLDAvis.gensim_models.prepare(lda7, corpus, dictionary)\n",
    "\n",
    "import pyLDAvis\n",
    "#pyLDAvis.enable_notebook()   # tell pyLDAvis to render inline\n",
    "#vis                          # simply evaluating the prepared vis object will display it\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jocelynshek/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 terms per topic (seeded):\n",
      "----------------------------------------\n",
      "Topic  0: israel, gaza, israeli, hamas, hezbollah, said, iran, military, lebanon, fire\n",
      "Topic  1: trump, president, abortion, donald, harris, states, biden, kamala, state, voters\n",
      "Topic  2: ukraine, russia, russian, president, ukrainian, tiktok, putin, moscow, china, said\n",
      "Topic  3: climate, change, heat, warming, hurricane, global, environmental, world, helene, planet\n",
      "Topic  4: china, electric, chinese, company, energy, boeing, steel, vehicles, tariffs, trade\n",
      "Topic  5: black, editor, readers, carter, also, museum, artist, died, beyonce, film\n",
      "Topic  6: inflation, rates, interest, reserve, federal, rate, market, bank, central, percent\n",
      "Topic  7: germany, europe, right, france, party, britain, european, government, chancellor, italy\n",
      "Topic  8: china, india, chinese, modi, hong, kong, pandas, narendra, trudeau, canada\n",
      "Topic  9: haiti, venezuela, africa, african, country, maduro, president, election, nicolas, kenya\n",
      "Topic 10: police, case, murder, guilty, death, accused, charges, plea, shooting, officers\n",
      "Topic 11: pricing, congestion, city, food, york, travel, hochul, transit, prices, price\n",
      "Topic 12: health, drug, drugs, ozempic, weight, women, prices, patients, covid, pharmacy\n",
      "Topic 13: olympics, olympic, doping, swimmers, games, paris, positive, chinese, baseball, athletes\n",
      "Topic 14: credit, bank, fees, card, banks, financial, federal, apple, billion, lawsuit\n",
      "Topic 15: pope, francis, vatican, bishop, church, budde, slur, mariann, italian, mercy\n",
      "Topic 16: baltic, cable, sabotage, cables, undersea, ship, authorities, severed, suspected, tanker\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "topic_number = 18\n",
    "\n",
    "# --- 0) Prep & preprocess as before ---\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess(text):\n",
    "    tokens = simple_preprocess(text or \"\", deacc=True)\n",
    "    return [t for t in tokens if t not in stop_words and len(t) > 3]\n",
    "\n",
    "docs_raw = more_info[\"combined_text\"].fillna(\"\").tolist()\n",
    "docs = [\" \".join(preprocess(doc)) for doc in docs_raw]\n",
    "\n",
    "# --- 1) Create your embedding model (deterministic by itself) ---\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# --- 2) Build a UMAP with a fixed seed ---\n",
    "umap_model = UMAP(\n",
    "    n_neighbors=15,\n",
    "    n_components=5,\n",
    "    min_dist=0.0,\n",
    "    metric=\"cosine\",\n",
    "    random_state=100,    # ← fix the seed here\n",
    "    low_memory=True\n",
    ")\n",
    "\n",
    "# --- 3) (Optional) Build an HDBSCAN with deterministic settings ---\n",
    "hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size=5, #og 10\n",
    "    metric=\"euclidean\",\n",
    "    cluster_selection_method=\"eom\",\n",
    "    prediction_data=True\n",
    "    # HDBSCAN doesn’t have a random_state, but this config is stable\n",
    ")\n",
    "\n",
    "# --- 4) Plug both into BERTopic ---\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=embedder,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    nr_topics=topic_number, #og 10\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# --- 5) Fit & transform (now reproducible!) ---\n",
    "topics, probs = topic_model.fit_transform(docs)\n",
    "\n",
    "# --- 6) Inspect your 10 topics again ---\n",
    "topic_info = topic_model.get_topic_info()\n",
    "valid_topics = topic_info[topic_info.Topic != -1].head(topic_number)[\"Topic\"].tolist()\n",
    "\n",
    "print(\"\\nTop 10 terms per topic (seeded):\\n\" + \"-\"*40)\n",
    "for t in valid_topics:\n",
    "    terms = topic_model.get_topic(t)\n",
    "    words = [w for w,_ in terms[:10]]\n",
    "    print(f\"Topic {t:2d}:\", \", \".join(words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying up to 18 topics with ≥50 docs:\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14] \n",
      "\n",
      "Top 10 terms per selected topic:\n",
      "--------------------------------------------------\n",
      "Topic  0 (2186 docs): israel, gaza, israeli, hamas, hezbollah, said, iran, military, lebanon, fire\n",
      "Topic  1 (1669 docs): trump, president, abortion, donald, harris, states, biden, kamala, state, voters\n",
      "Topic  2 (1218 docs): ukraine, russia, russian, president, ukrainian, tiktok, putin, moscow, china, said\n",
      "Topic  3 (477 docs): climate, change, heat, warming, hurricane, global, environmental, world, helene, planet\n",
      "Topic  4 (430 docs): china, electric, chinese, company, energy, boeing, steel, vehicles, tariffs, trade\n",
      "Topic  5 (401 docs): black, editor, readers, carter, also, museum, artist, died, beyonce, film\n",
      "Topic  6 (367 docs): inflation, rates, interest, reserve, federal, rate, market, bank, central, percent\n",
      "Topic  7 (263 docs): germany, europe, right, france, party, britain, european, government, chancellor, italy\n",
      "Topic  8 (188 docs): china, india, chinese, modi, hong, kong, pandas, narendra, trudeau, canada\n",
      "Topic  9 (187 docs): haiti, venezuela, africa, african, country, maduro, president, election, nicolas, kenya\n",
      "Topic 10 (180 docs): police, case, murder, guilty, death, accused, charges, plea, shooting, officers\n",
      "Topic 11 (175 docs): pricing, congestion, city, food, york, travel, hochul, transit, prices, price\n",
      "Topic 12 (120 docs): health, drug, drugs, ozempic, weight, women, prices, patients, covid, pharmacy\n",
      "Topic 13 (87 docs): olympics, olympic, doping, swimmers, games, paris, positive, chinese, baseball, athletes\n",
      "Topic 14 (75 docs): credit, bank, fees, card, banks, financial, federal, apple, billion, lawsuit\n"
     ]
    }
   ],
   "source": [
    "# Set your parameters up top\n",
    "topic_number = 18        # how many topics to display at most\n",
    "min_topic_size = 50      # minimum doc‐count per topic\n",
    "outlier_id = -1\n",
    "\n",
    "# 1) Get the topic info table\n",
    "topic_info = topic_model.get_topic_info()  \n",
    "# Columns: ['Topic', 'Count', 'Name']\n",
    "\n",
    "# 2) Identify “big” topics (≥ min_topic_size) and drop outliers\n",
    "big_topics = topic_info[\n",
    "    (topic_info.Count >= min_topic_size) &\n",
    "    (topic_info.Topic != outlier_id)\n",
    "][\"Topic\"].tolist()\n",
    "\n",
    "# 3) If there are more than topic_number, cut off\n",
    "selected_topics = big_topics[:topic_number]\n",
    "\n",
    "print(f\"Displaying up to {topic_number} topics with ≥{min_topic_size} docs:\")\n",
    "print(selected_topics, \"\\n\")\n",
    "\n",
    "# 4) Print top-10 terms for each selected topic\n",
    "print(\"Top 10 terms per selected topic:\\n\" + \"-\"*50)\n",
    "for t in selected_topics:\n",
    "    count = topic_info.loc[topic_info.Topic == t, \"Count\"].values[0]\n",
    "    terms = topic_model.get_topic(t)   # [(word, score), ...]\n",
    "    words = [w for w,_ in terms[:10]]\n",
    "    print(f\"Topic {t:2d} ({count} docs): {', '.join(words)}\")\n",
    "\n",
    "# 5) Visualize only those selected topics\n",
    "fig_map = topic_model.visualize_topics(topics=selected_topics)\n",
    "#fig_map.show()\n",
    "\n",
    "fig_bar = topic_model.visualize_barchart(\n",
    "    topics=selected_topics,\n",
    "    top_n_topics=len(selected_topics),\n",
    "    n_words=10\n",
    ")\n",
    "#fig_bar.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jocelynshek/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 15 topic networks with size & group to joc-data/networks.json\n"
     ]
    }
   ],
   "source": [
    "#ok actually works fr may 12\n",
    "\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import networkx as nx\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download standard stopwords if you haven't already\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Start with standard English stopwords\n",
    "custom_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# Add your domain-specific words that you find too generic or uninformative\n",
    "additional_stopwords = [\n",
    "    # Generic political names/terms (you can add or remove as needed)\n",
    "    \"york times\", \"years ago\", \"year old\", \"today episode\",\n",
    "    \"guest\", \"essay\",\n",
    "    \"content_kicker\", \"kicker\", \"print_headline\"\n",
    "]\n",
    "\n",
    "# Combine the lists\n",
    "custom_stopwords = custom_stopwords.union(set(additional_stopwords))\n",
    "\n",
    "#originally included overlapping phrases: manual check for famous people\n",
    "ENTITY_MAP = {\n",
    "    \"prime minister benjamin\": \"benjamin netanyahu\",\n",
    "    \"minister benjmain netanyahu\": \"benjamin netanyahu\",\n",
    "    \"minister benjamin\": \"benjamin netanyahu\",\n",
    "    \"benjamin netanyahu israel\": \"benjamin netanyahu\",\n",
    "    \"president donald\": \"donald trump\",\n",
    "    \"president trump\": \"donald trump\",\n",
    "    \"former donald\": \"donald trump\",\n",
    "    \"former donald trump\": \"donald trump\",\n",
    "    \"president elect donald\": \"donald trump\",\n",
    "    \"elect donald trump\": \"donald trump\",\n",
    "    \"donald trump trump\": \"donald trump\",\n",
    "    \"trump trump\": \"donald trump\",\n",
    "    \"vice president kamala\": \"kamala harris\",\n",
    "    \"president kamala harris\": \"kamala harris\",\n",
    "    \"mayor eric\": \"eric adams\",\n",
    "    \"mayor eric adams\": \"eric adams\",\n",
    "    \"state antony\": \"antony blinken\",\n",
    "    \"secretary state antony\": \"antony blinken\",\n",
    "    \"street journal\": \"wall street journal\",\n",
    "    \"president volodymyr zelensky\": \"volodymyr zelensky\",\n",
    "    \"president volodymyr\": \"volodymyr zelensky\",\n",
    "    \"volodymyr zelensky ukraine\": \"volodymyr zelensky\",\n",
    "    \"volodymyr ukraine\": \"volodymyr zelensky\",\n",
    "    \"president vladimir putin\": \"vladimir putin\",\n",
    "    \"president vladimir\": \"volodymyr zelensky\",\n",
    "    \"volodymyr ukraine\": \"volodymyr zelensky\",\n",
    "    \"minister justin\": \"justin trudeau\",\n",
    "    \"prime minister justin\": \"justin trudeau\",\n",
    "    \"minister justin trudeau\": \"justin trudeau\",\n",
    "    \"trudeau canada\": \"justin trudeau\",\n",
    "    \"justin trudeau canada\": \"justin trudeau\",\n",
    "    # etc.\n",
    "}\n",
    "\n",
    "import re\n",
    "\n",
    "def remove_duplicate_phrases(text):\n",
    "    return re.sub(r'\\b(\\w+)\\s+\\1\\b', r'\\1', text)\n",
    "\n",
    "# Update normalize_doc\n",
    "def normalize_doc(text):\n",
    "    for k, v in ENTITY_MAP.items():\n",
    "        pattern = r'\\b' + re.escape(k) + r'\\b'\n",
    "        text = re.sub(pattern, v, text, flags=re.IGNORECASE)\n",
    "    text = remove_duplicate_phrases(text)\n",
    "    return text\n",
    "\n",
    "# --- Updated build_network with size & community group ---\n",
    "def build_network(topic_id, docs, topics, min_cooccurrence=3, top_n_phrases=50):\n",
    "    # 1) Filter docs for this topic\n",
    "    topic_docs = [docs[i] for i, t in enumerate(topics) if t == topic_id]\n",
    "    topic_docs = [normalize_doc(doc) for doc in topic_docs]\n",
    "    \n",
    "    # 2) Extract bigrams/trigrams\n",
    "    \n",
    "    vec = CountVectorizer(ngram_range=(2, 3), stop_words=list(custom_stopwords), max_features=top_n_phrases)\n",
    "    \n",
    "    X = vec.fit_transform(topic_docs)\n",
    "    phrases = vec.get_feature_names_out()\n",
    "    \n",
    "    # 3) Calculate frequencies\n",
    "    freqs = dict(zip(phrases, X.sum(axis=0).A1))\n",
    "    \n",
    "    # 4) Build co-occurrence counts\n",
    "    co = defaultdict(int)\n",
    "    for doc in topic_docs:\n",
    "        present = [p for p in phrases if p in doc]\n",
    "        for i in range(len(present)):\n",
    "            for j in range(i+1, len(present)):\n",
    "                a, b = sorted([present[i], present[j]])\n",
    "                co[(a, b)] += 1\n",
    "    \n",
    "    # 5) Create graph\n",
    "    G = nx.Graph()\n",
    "    for phrase in phrases:\n",
    "        G.add_node(phrase, frequency=int(freqs.get(phrase, 0)))\n",
    "    for (a, b), w in co.items():\n",
    "        if w >= min_cooccurrence:\n",
    "            G.add_edge(a, b, weight=int(w))\n",
    "    \n",
    "    # 6) Community detection for grouping/color\n",
    "    communities = list(nx.community.greedy_modularity_communities(G))\n",
    "    comm_map = {}\n",
    "    for idx, comm in enumerate(communities):\n",
    "        for node in comm:\n",
    "            comm_map[node] = idx\n",
    "    \n",
    "    # 7) Serialize nodes and links with size & group\n",
    "    nodes = []\n",
    "    for node, data in G.nodes(data=True):\n",
    "        nodes.append({\n",
    "            \"id\": node,\n",
    "            \"size\": data[\"frequency\"],        # node size\n",
    "            \"group\": comm_map.get(node, -1)   # community id\n",
    "        })\n",
    "    links = []\n",
    "    for u, v, data in G.edges(data=True):\n",
    "        links.append({\n",
    "            \"source\": u,\n",
    "            \"target\": v,\n",
    "            \"value\": data[\"weight\"]           # edge weight\n",
    "        })\n",
    "    \n",
    "    return {\"topic\": topic_id, \"nodes\": nodes, \"links\": links}\n",
    "\n",
    "# --- Build and write JSON ---\n",
    "# Assume `docs` and `topics` (from topic_model.transform) defined, and `selected_topics` list exists\n",
    "networks = [build_network(tid, docs, topics, min_cooccurrence=5, top_n_phrases=50)\n",
    "            for tid in selected_topics]\n",
    "\n",
    "with open(\"joc-data/networks.json\", \"w\") as f:\n",
    "    json.dump(networks, f, indent=2)\n",
    "\n",
    "print(f\"Wrote {len(networks)} topic networks with size & group to joc-data/networks.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_meta = []\n",
    "\n",
    "for topic_id in valid_topics:\n",
    "    words = [w for w, _ in topic_model.get_topic(topic_id)[:10]]\n",
    "    terms = topic_model.get_topic(topic_id)[:10]  # top 10 (word, weight) pairs\n",
    "    top_words = [{\"word\": word, \"value\": float(score)} for word, score in terms]\n",
    "    label = \", \".join(words)\n",
    "    \n",
    "    # Get frequency (i.e. number of documents assigned to this topic)\n",
    "    count = topic_info[topic_info[\"Topic\"] == topic_id][\"Count\"].values[0]\n",
    "    \n",
    "    # Get topic embedding coords — assumes 2D\n",
    "    idx = topic_info[topic_info[\"Topic\"] == topic_id].index[0]\n",
    "    x, y = topic_model.topic_embeddings_[idx][:2]\n",
    "\n",
    "    topics_meta.append({\n",
    "        \"id\": topic_id,\n",
    "        \"label\": label,\n",
    "        \"topWords\": top_words,\n",
    "        \"count\": int(count),\n",
    "        \"x\": float(x),\n",
    "        \"y\": float(y)\n",
    "    })\n",
    "\n",
    "with open(\"joc-data/topics.json\",\"w\") as f:\n",
    "    json.dump(topics_meta, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_raw = more_info[\"combined_text\"].fillna(\"\").tolist()\n",
    "docs = [\" \".join(preprocess(doc)) for doc in docs_raw]\n",
    "\n",
    "# Create a filtered version of more_info to match topics\n",
    "more_info_filtered = more_info.loc[:len(topics) - 1].copy()\n",
    "\n",
    "topic_supergroups = {\n",
    "    0: \"International geopolitics\",\n",
    "    1: \"Internal politics\",\n",
    "    2: \"International geopolitics\",\n",
    "    3: \"Climate\",\n",
    "    4: \"International geopolitics\",\n",
    "    5: \"Culture\",\n",
    "    6: \"Internal politics\",\n",
    "    7: \"International geopolitics\",\n",
    "    8: \"International geopolitics\",\n",
    "    9: \"International geopolitics\",\n",
    "    10: \"Internal politics\",\n",
    "    11: \"Internal politics\",\n",
    "    12: \"Culture\",\n",
    "    13: \"Culture\",\n",
    "    14: \"Internal politics\",\n",
    "    15: \"Culture\",\n",
    "    16: \"Internal politics\"\n",
    "}\n",
    "\n",
    "manualLabels = {\n",
    "    0: \"Israeli–Palestinian conflict\",\n",
    "    1: \"US Politics & Elections\",\n",
    "    2: \"Russia-Ukraine War, Global Politics\",\n",
    "    3: \"Climate Change & Extreme Weather\",\n",
    "    4: \"China, Trade, & Energy Industry\",\n",
    "    5: \"Culture, Arts, Obituaries\",\n",
    "    6: \"Economy\",\n",
    "    7: \"European Politics\",\n",
    "    8: \"India-China Relations & Global Leaders\",\n",
    "    9: \"Global South: Haiti, Africa, Venezuela\",\n",
    "    10: \"Crime, Trials & Policing\",\n",
    "    11: \"Urban Policy\",\n",
    "    12: \"Health\",\n",
    "    13: \"International Sports\",\n",
    "    14: \"Banking\",\n",
    "    15: \"Pope, Church, & Religion\",\n",
    "    16: \"Security\"\n",
    "}\n",
    "\n",
    "topics_meta = []\n",
    "\n",
    "for topic_id in valid_topics:\n",
    "    words = [w for w, _ in topic_model.get_topic(topic_id)[:10]]\n",
    "\n",
    "\n",
    "\n",
    "    # Get top 10 words for topic\n",
    "    terms = topic_model.get_topic(topic_id)[:10]\n",
    "    top_words = []\n",
    "\n",
    "    # Find indices of docs assigned to this topic\n",
    "    doc_indices = [i for i, t in enumerate(topics) if t == topic_id]\n",
    "    topic_docs = [docs[i] for i in doc_indices]\n",
    "    num_docs = len(topic_docs)\n",
    "\n",
    "    for word, _ in terms:\n",
    "        count = sum(1 for doc in topic_docs if word in doc.split())\n",
    "        percent = count / num_docs if num_docs else 0\n",
    "        top_words.append({\n",
    "            \"word\": word,\n",
    "            \"value\": round(percent * 100, 1)  # as percentage\n",
    "        })\n",
    "\n",
    "\n",
    "\n",
    "    label = \", \".join(words)\n",
    "    count = topic_info[topic_info[\"Topic\"] == topic_id][\"Count\"].values[0]\n",
    "    \n",
    "\n",
    "    topics_meta.append({\n",
    "        \"id\": topic_id,\n",
    "        \"label\": label,\n",
    "        \"count\": int(count),\n",
    "        \"topWords\": top_words\n",
    "    })\n",
    "\n",
    "\n",
    "for topic in topics_meta:\n",
    "    topic[\"group\"] = topic_supergroups.get(topic[\"id\"], \"Other\")\n",
    "\n",
    "for topic in topics_meta:\n",
    "    topic[\"manualLabel\"] = manualLabels.get(topic[\"id\"], \"Other\")\n",
    "\n",
    "with open(\"joc-data/topics.json\", \"w\") as f:\n",
    "    json.dump(topics_meta, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (venv312)",
   "language": "python",
   "name": "venv312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
